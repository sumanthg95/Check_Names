{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem No.1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "By Sumanth Gopalkrishna & Tanukrishna Chetia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### importing the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### creating the input array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Input:\n",
      "[[0 0 0 0]\n",
      " [0 0 0 1]\n",
      " [0 0 1 0]\n",
      " [0 0 1 1]\n",
      " [0 1 0 0]\n",
      " [0 1 0 1]\n",
      " [0 1 1 0]\n",
      " [0 1 1 1]\n",
      " [1 0 0 0]\n",
      " [1 0 0 1]\n",
      " [1 0 1 0]\n",
      " [1 0 1 1]\n",
      " [1 1 0 0]\n",
      " [1 1 0 1]\n",
      " [1 1 1 0]\n",
      " [1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[0,0,0,0],[0,0,0,1],[0,0,1,0],[0,0,1,1],[0,1,0,0],[0,1,0,1],\n",
    "              [0,1,1,0],[0,1,1,1],[1,0,0,0],[1,0,0,1],[1,0,1,0],[1,0,1,1],[1,1,0,0],[1,1,0,1],[1,1,1,0],[1,1,1,1]])\n",
    "\n",
    "print ('\\n Input:')\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### creating the output array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Actual Output:\n",
      "[[0 1 1 0 1 0 0 1 1 0 0 1 0 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "y=np.array([[0],[1],[1],[0],[1],[0],[0],[1],[1],[0],[0],[1],[0],[1],[1],[0]]).T\n",
    "print ('\\n Actual Output:')\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization of weights and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputneurons,hiddenlayerneurons,outputneurons=4,4,1\n",
    "W1 = np.random.uniform(-1,1, size=(hiddenlayerneurons,inputneurons))\n",
    "B1 = np.random.uniform(-1,1, size=(hiddenlayerneurons,1))\n",
    "W2 = np.random.uniform(-1,1, size=(outputneurons,hiddenlayerneurons))\n",
    "B2 = np.random.uniform(-1,1, size=(outputneurons,1)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### defining the Sigmoid Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid (x):\n",
    "    return 1/(1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forwardpropogation(X,y,W1,B1,W2,B2):\n",
    "     A1=np.dot(W1,X.T)+B1\n",
    "     initial_output= sigmoid(A1)\n",
    "     A2=np.dot(W2,initial_output)+B2\n",
    "     predicted_output= sigmoid(A2)\n",
    "     cache = (A1,initial_output,W1,B1, A2 ,predicted_output ,W2, B2)\n",
    "     logprobs = np.multiply(np.log(predicted_output), y) + np.multiply(np.log(1 - predicted_output), (1 - y))\n",
    "     cost = -np.sum(logprobs) / X.shape[1]\n",
    "     return cost, cache, predicted_output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backwardpropogation(X,Y,cache):\n",
    "    (A1,initial_output , W1,B1, A2 , predicted_output ,W2, B2)=cache\n",
    "    dZ2=predicted_output-y\n",
    "    dW2=np.dot(dZ2,initial_output.T)/X.shape[1]\n",
    "    dB2=np.sum(dZ2,axis=1 ,keepdims=True)/X.shape[1]\n",
    "    \n",
    "    dA1 = np.dot(W2.T, dZ2)\n",
    "    dZ1 = np.multiply(dA1, initial_output * (1- initial_output))\n",
    "    dW1 = np.dot(dZ1, X) / X.shape[1]\n",
    "    dB1 = np.sum(dZ1, axis = 1, keepdims = True) / X.shape[1]\n",
    "    \n",
    "    gradients = {\"dZ2\": dZ2, \"dW2\": dW2, \"dB2\": dB2,\n",
    "                 \"dZ1\": dZ1, \"dW1\": dW1, \"dB1\": dB1}\n",
    "    return gradients\n",
    "  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateParameters(W1,W2,B1,B2, gradients, learningRate):\n",
    "    W1 =W1 - learningRate * gradients[\"dW1\"]\n",
    "    W2 =W2 - learningRate * gradients[\"dW2\"]\n",
    "    B1= B1 - learningRate * gradients[\"dB1\"]\n",
    "    B2 =B2 - learningRate * gradients[\"dB2\"]\n",
    "    return W1,W2,B1,B2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting Learning curving and Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00004699 0.99983491 0.99987635 0.00003066 0.99983491 0.00053172\n",
      "  0.00003065 0.99988567 0.9998763  0.00003062 0.00044338 0.99991766\n",
      "  0.0000306  0.99988569 0.99991768 0.00002502]]\n"
     ]
    }
   ],
   "source": [
    "#plotting learning curve\n",
    "from matplotlib import pyplot as plt\n",
    "epoch = 800000\n",
    "learningRate = 0.05\n",
    "losses = np.zeros((epoch, 1))\n",
    "  \n",
    "for i in range(epoch):\n",
    "    losses[i, 0], cache, A2 = forwardpropogation(X, y,W1,B1,W2,B2 )\n",
    "    gradients = backwardpropogation(X, y, cache)\n",
    "    W1,W2,B1,B2 = updateParameters(W1,W2,B1,B2, gradients, learningRate)\n",
    "  \n",
    "# Evaluating the performance\n",
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"EPOCHS\")\n",
    "plt.ylabel(\"Loss value\")\n",
    "plt.show()\n",
    "\n",
    "cost, _,predicted_output = forwardpropogation(X, y, W1,B1,W2,B2)\n",
    "np.set_printoptions(suppress=True)\n",
    "print(predicted_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00002216 0.99994301 0.99994678 0.00001518 0.99994301 0.00017958\n",
      "  0.00001518 0.99995884 0.99994677 0.00001517 0.00016894 0.9999629\n",
      "  0.00001516 0.99995883 0.9999629  0.00001273]]\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "epoch = 500000\n",
    "learningRate = 0.1\n",
    "losses = np.zeros((epoch, 1))\n",
    "  \n",
    "for i in range(epoch):\n",
    "    losses[i, 0], cache, A2 = forwardpropogation(X, y,W1,B1,W2,B2 )\n",
    "    gradients = backwardpropogation(X, y, cache)\n",
    "    W1,W2,B1,B2 = updateParameters(W1,W2,B1,B2, gradients, learningRate)\n",
    "  \n",
    "# Evaluating the performance\n",
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"EPOCHS\")\n",
    "plt.ylabel(\"Loss value\")\n",
    "plt.show()\n",
    "\n",
    "cost, _,predicted_output = forwardpropogation(X, y, W1,B1,W2,B2)\n",
    "np.set_printoptions(suppress=True)\n",
    "print(predicted_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "epoch = 400000\n",
    "learningRate = 0.15\n",
    "losses = np.zeros((epoch, 1))\n",
    "  \n",
    "for i in range(epoch):\n",
    "    losses[i, 0], cache, A2 = forwardpropogation(X, y,W1,B1,W2,B2 )\n",
    "    gradients = backwardpropogation(X, y, cache)\n",
    "    W1,W2,B1,B2 = updateParameters(W1,W2,B1,B2, gradients, learningRate)\n",
    "  \n",
    "# Evaluating the performance\n",
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"EPOCHS\")\n",
    "plt.ylabel(\"Loss value\")\n",
    "plt.show()\n",
    "\n",
    "cost, _,predicted_output = forwardpropogation(X, y, W1,B1,W2,B2)\n",
    "np.set_printoptions(suppress=True)\n",
    "print(predicted_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "epoch = 300000\n",
    "learningRate = 0.20\n",
    "losses = np.zeros((epoch, 1))\n",
    "  \n",
    "for i in range(epoch):\n",
    "    losses[i, 0], cache, A2 = forwardpropogation(X, y,W1,B1,W2,B2 )\n",
    "    gradients = backwardpropogation(X, y, cache)\n",
    "    W1,W2,B1,B2 = updateParameters(W1,W2,B1,B2, gradients, learningRate)\n",
    "  \n",
    "# Evaluating the performance\n",
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"EPOCHS\")\n",
    "plt.ylabel(\"Loss value\")\n",
    "plt.show()\n",
    "\n",
    "cost, _,predicted_output = forwardpropogation(X, y, W1,B1,W2,B2)\n",
    "np.set_printoptions(suppress=True)\n",
    "print(predicted_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "epoch = 200000\n",
    "learningRate = 0.25\n",
    "losses = np.zeros((epoch, 1))\n",
    "  \n",
    "for i in range(epoch):\n",
    "    losses[i, 0], cache, A2 = forwardpropogation(X, y,W1,B1,W2,B2 )\n",
    "    gradients = backwardpropogation(X, y, cache)\n",
    "    W1,W2,B1,B2 = updateParameters(W1,W2,B1,B2, gradients, learningRate)\n",
    "  \n",
    "# Evaluating the performance\n",
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"EPOCHS\")\n",
    "plt.ylabel(\"Loss value\")\n",
    "plt.show()\n",
    "\n",
    "cost, _,predicted_output = forwardpropogation(X, y, W1,B1,W2,B2)\n",
    "np.set_printoptions(suppress=True)\n",
    "print(predicted_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "epoch = 100000\n",
    "learningRate = 0.3\n",
    "losses = np.zeros((epoch, 1))\n",
    "  \n",
    "for i in range(epoch):\n",
    "    losses[i, 0], cache, A2 = forwardpropogation(X, y,W1,B1,W2,B2 )\n",
    "    gradients = backwardpropogation(X, y, cache)\n",
    "    W1,W2,B1,B2 = updateParameters(W1,W2,B1,B2, gradients, learningRate)\n",
    "  \n",
    "# Evaluating the performance\n",
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"EPOCHS\")\n",
    "plt.ylabel(\"Loss value\")\n",
    "plt.show()\n",
    "\n",
    "cost, _,predicted_output = forwardpropogation(X, y, W1,B1,W2,B2)\n",
    "np.set_printoptions(suppress=True)\n",
    "print(predicted_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "epoch = 90000\n",
    "learningRate = 0.35\n",
    "losses = np.zeros((epoch, 1))\n",
    "  \n",
    "for i in range(epoch):\n",
    "    losses[i, 0], cache, A2 = forwardpropogation(X, y,W1,B1,W2,B2 )\n",
    "    gradients = backwardpropogation(X, y, cache)\n",
    "    W1,W2,B1,B2 = updateParameters(W1,W2,B1,B2, gradients, learningRate)\n",
    "  \n",
    "# Evaluating the performance\n",
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"EPOCHS\")\n",
    "plt.ylabel(\"Loss value\")\n",
    "plt.show()\n",
    "\n",
    "cost, _,predicted_output = forwardpropogation(X, y, W1,B1,W2,B2)\n",
    "np.set_printoptions(suppress=True)\n",
    "print(predicted_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "epoch = 80000\n",
    "learningRate = 0.4\n",
    "losses = np.zeros((epoch, 1))\n",
    "  \n",
    "for i in range(epoch):\n",
    "    losses[i, 0], cache, A2 = forwardpropogation(X, y,W1,B1,W2,B2 )\n",
    "    gradients = backwardpropogation(X, y, cache)\n",
    "    W1,W2,B1,B2 = updateParameters(W1,W2,B1,B2, gradients, learningRate)\n",
    "  \n",
    "# Evaluating the performance\n",
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"EPOCHS\")\n",
    "plt.ylabel(\"Loss value\")\n",
    "plt.show()\n",
    "\n",
    "cost, _,predicted_output = forwardpropogation(X, y, W1,B1,W2,B2)\n",
    "np.set_printoptions(suppress=True)\n",
    "print(predicted_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "epoch = 70000\n",
    "learningRate = 0.45\n",
    "losses = np.zeros((epoch, 1))\n",
    "  \n",
    "for i in range(epoch):\n",
    "    losses[i, 0], cache, A2 = forwardpropogation(X, y,W1,B1,W2,B2 )\n",
    "    gradients = backwardpropogation(X, y, cache)\n",
    "    W1,W2,B1,B2 = updateParameters(W1,W2,B1,B2, gradients, learningRate)\n",
    "  \n",
    "# Evaluating the performance\n",
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"EPOCHS\")\n",
    "plt.ylabel(\"Loss value\")\n",
    "plt.show()\n",
    "\n",
    "cost, _,predicted_output = forwardpropogation(X, y, W1,B1,W2,B2)\n",
    "np.set_printoptions(suppress=True)\n",
    "print(predicted_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "epoch = 60000\n",
    "learningRate = 0.5\n",
    "losses = np.zeros((epoch, 1))\n",
    "  \n",
    "for i in range(epoch):\n",
    "    losses[i, 0], cache, A2 = forwardpropogation(X, y,W1,B1,W2,B2 )\n",
    "    gradients = backwardpropogation(X, y, cache)\n",
    "    W1,W2,B1,B2 = updateParameters(W1,W2,B1,B2, gradients, learningRate)\n",
    "  \n",
    "# Evaluating the performance\n",
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"EPOCHS\")\n",
    "plt.ylabel(\"Loss value\")\n",
    "plt.show()\n",
    "\n",
    "cost, _,predicted_output = forwardpropogation(X, y, W1,B1,W2,B2)\n",
    "np.set_printoptions(suppress=True)\n",
    "print(predicted_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a momentum term in the weight update with α = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = np.array([[0,0,0,0],[0,0,0,1],[0,0,1,0],[0,0,1,1],[0,1,0,0],[0,1,0,1],[0,1,1,0],[0,1,1,1],[1,0,0,0],[1,0,0,1],[1,0,1,0],[1,0,1,1],[1,1,0,0],[1,1,0,1],[1,1,1,0],[1,1,1,1]])\n",
    "y = np.array([[0],[1],[1],[0],[1],[0],[0],[1],[1],[0],[0],[1],[0],[1],[1],[0]]).T\n",
    "\n",
    "inputneurons,hiddenlayerneurons,outputneurons=4,4,1\n",
    "W1 = np.random.uniform(-1,1, size=(hiddenlayerneurons,inputneurons))\n",
    "B1 = np.random.uniform(-1,1, size=(hiddenlayerneurons,1))\n",
    "W2 = np.random.uniform(-1,1, size=(outputneurons,hiddenlayerneurons))\n",
    "B2 = np.random.uniform(-1,1, size=(outputneurons,1)) \n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1/(1 + np.exp(-z))\n",
    "def sigmoid_derivative(z):\n",
    "    return z*(1-z)\n",
    "\n",
    "def forwardpropogation(X,y,W1,B1,W2,B2):\n",
    "     A1=np.dot(W1,X.T)+B1\n",
    "     initial_output= sigmoid(A1)\n",
    "     A2=np.dot(W2,initial_output)+B2\n",
    "     predicted_output= sigmoid(A2)\n",
    "     cache = (A1,initial_output,W1,B1, A2 ,predicted_output ,W2, B2)\n",
    "     logprobs = np.multiply(np.log(predicted_output), y) + np.multiply(np.log(1 - predicted_output), (1 - y))\n",
    "     cost = -np.sum(logprobs) / X.shape[1]\n",
    "     return cost, cache, predicted_output\n",
    "    \n",
    "\n",
    "def backwardpropogation(X,Y,cache):\n",
    "    (A1,initial_output , W1,B1, A2 , predicted_output ,W2, B2)=cache\n",
    "    dZ2=predicted_output-y\n",
    "    dW2=(np.dot(dZ2,initial_output.T)/X.shape[1])\n",
    "    dW2=  dW2 + 0.9*dW2\n",
    "    dB2=np.sum(dZ2,axis=1 ,keepdims=True)/X.shape[1]\n",
    "    \n",
    "    dA1 = np.dot(W2.T, dZ2)\n",
    "    dZ1 = np.multiply(dA1, initial_output * (1- initial_output))\n",
    "    dW1 = (np.dot(dZ1, X) / X.shape[1]) \n",
    "    dW1=  dW1 + 0.9*dW1\n",
    "    dB1 = np.sum(dZ1, axis = 1, keepdims = True) / X.shape[1]\n",
    "    \n",
    "    gradients = {\"dZ2\": dZ2, \"dW2\": dW2, \"dB2\": dB2,\n",
    "                 \"dZ1\": dZ1, \"dW1\": dW1, \"dB1\": dB1}\n",
    "    return gradients\n",
    "  \n",
    "    \n",
    "\n",
    "def updateParameters(W1,W2,B1,B2, gradients, learningRate):\n",
    "  \n",
    "    W1 =W1 - learningRate * gradients[\"dW1\"]\n",
    "    W2 =W2 - learningRate * gradients[\"dW2\"]\n",
    "    B1= B1 - learningRate * gradients[\"dB1\"]\n",
    "    B2 =B2 - learningRate * gradients[\"dB2\"]\n",
    "    return W1,W2,B1,B2\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "epoch = 100000\n",
    "learningRate = 0.05\n",
    "losses = np.zeros((epoch, 1))\n",
    "  \n",
    "for i in range(epoch):\n",
    "    losses[i, 0], cache, A2 = forwardpropogation(X, y,W1,B1,W2,B2 )\n",
    "    gradients = backwardpropogation(X, y, cache)\n",
    "    W1,W2,B1,B2 = updateParameters(W1,W2,B1,B2, gradients, learningRate)\n",
    "  \n",
    "# Evaluating the performance\n",
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"EPOCHS\")\n",
    "plt.ylabel(\"Loss value\")\n",
    "p2 = plt.show()\n",
    "\n",
    "cost, _,predicted_output = forwardpropogation(X, y, W1,B1,W2,B2)\n",
    "np.set_printoptions(suppress=True)\n",
    "print(predicted_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.array([[0,0,0,0],[0,0,0,1],[0,0,1,0],[0,0,1,1],[0,1,0,0],[0,1,0,1],[0,1,1,0],[0,1,1,1],[1,0,0,0],[1,0,0,1],[1,0,1,0],[1,0,1,1],[1,1,0,0],[1,1,0,1],[1,1,1,0],[1,1,1,1]])\n",
    "y = np.array([[0],[1],[1],[0],[1],[0],[0],[1],[1],[0],[0],[1],[0],[1],[1],[0]]).T\n",
    "\n",
    "inputneurons,hiddenlayerneurons,outputneurons=4,4,1\n",
    "W1 = np.random.uniform(-1,1, size=(hiddenlayerneurons,inputneurons))\n",
    "B1 = np.random.uniform(-1,1, size=(hiddenlayerneurons,1))\n",
    "W2 = np.random.uniform(-1,1, size=(outputneurons,hiddenlayerneurons))\n",
    "B2 = np.random.uniform(-1,1, size=(outputneurons,1)) \n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1/(1 + np.exp(-z))\n",
    "def sigmoid_derivative(z):\n",
    "    return z*(1-z)\n",
    "\n",
    "def forwardpropogation(X,y,W1,B1,W2,B2):\n",
    "     A1=np.dot(W1,X.T)+B1\n",
    "     initial_output= sigmoid(A1)\n",
    "     A2=np.dot(W2,initial_output)+B2\n",
    "     predicted_output= sigmoid(A2)\n",
    "     cache = (A1,initial_output,W1,B1, A2 ,predicted_output ,W2, B2)\n",
    "     logprobs = np.multiply(np.log(predicted_output), y) + np.multiply(np.log(1 - predicted_output), (1 - y))\n",
    "     cost = -np.sum(logprobs) / X.shape[1]\n",
    "     return cost, cache, predicted_output\n",
    "    \n",
    "\n",
    "def backwardpropogation(X,Y,cache):\n",
    "    (A1,initial_output , W1,B1, A2 , predicted_output ,W2, B2)=cache\n",
    "    dZ2=predicted_output-y\n",
    "    dW2=(np.dot(dZ2,initial_output.T)/X.shape[1])\n",
    "    dW2=  dW2 + 0.9*dW2\n",
    "    dB2=np.sum(dZ2,axis=1 ,keepdims=True)/X.shape[1]\n",
    "    \n",
    "    dA1 = np.dot(W2.T, dZ2)\n",
    "    dZ1 = np.multiply(dA1, initial_output * (1- initial_output))\n",
    "    dW1 = (np.dot(dZ1, X) / X.shape[1]) \n",
    "    dW1=  dW1 + 0.9*dW1\n",
    "    dB1 = np.sum(dZ1, axis = 1, keepdims = True) / X.shape[1]\n",
    "    \n",
    "    gradients = {\"dZ2\": dZ2, \"dW2\": dW2, \"dB2\": dB2,\n",
    "                 \"dZ1\": dZ1, \"dW1\": dW1, \"dB1\": dB1}\n",
    "    return gradients\n",
    "  \n",
    "    \n",
    "\n",
    "def updateParameters(W1,W2,B1,B2, gradients, learningRate):\n",
    "  \n",
    "    W1 =W1 - learningRate * gradients[\"dW1\"]\n",
    "    W2 =W2 - learningRate * gradients[\"dW2\"]\n",
    "    B1= B1 - learningRate * gradients[\"dB1\"]\n",
    "    B2 =B2 - learningRate * gradients[\"dB2\"]\n",
    "    return W1,W2,B1,B2\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "epoch = 90000\n",
    "learningRate = 0.1\n",
    "losses = np.zeros((epoch, 1))\n",
    "  \n",
    "for i in range(epoch):\n",
    "    losses[i, 0], cache, A2 = forwardpropogation(X, y,W1,B1,W2,B2 )\n",
    "    gradients = backwardpropogation(X, y, cache)\n",
    "    W1,W2,B1,B2 = updateParameters(W1,W2,B1,B2, gradients, learningRate)\n",
    "  \n",
    "# Evaluating the performance\n",
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"EPOCHS\")\n",
    "plt.ylabel(\"Loss value\")\n",
    "p2 = plt.show()\n",
    "\n",
    "cost, _,predicted_output = forwardpropogation(X, y, W1,B1,W2,B2)\n",
    "np.set_printoptions(suppress=True)\n",
    "print(predicted_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.array([[0,0,0,0],[0,0,0,1],[0,0,1,0],[0,0,1,1],[0,1,0,0],[0,1,0,1],[0,1,1,0],[0,1,1,1],[1,0,0,0],[1,0,0,1],[1,0,1,0],[1,0,1,1],[1,1,0,0],[1,1,0,1],[1,1,1,0],[1,1,1,1]])\n",
    "y = np.array([[0],[1],[1],[0],[1],[0],[0],[1],[1],[0],[0],[1],[0],[1],[1],[0]]).T\n",
    "\n",
    "inputneurons,hiddenlayerneurons,outputneurons=4,4,1\n",
    "W1 = np.random.uniform(-1,1, size=(hiddenlayerneurons,inputneurons))\n",
    "B1 = np.random.uniform(-1,1, size=(hiddenlayerneurons,1))\n",
    "W2 = np.random.uniform(-1,1, size=(outputneurons,hiddenlayerneurons))\n",
    "B2 = np.random.uniform(-1,1, size=(outputneurons,1)) \n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1/(1 + np.exp(-z))\n",
    "def sigmoid_derivative(z):\n",
    "    return z*(1-z)\n",
    "\n",
    "def forwardpropogation(X,y,W1,B1,W2,B2):\n",
    "     A1=np.dot(W1,X.T)+B1\n",
    "     initial_output= sigmoid(A1)\n",
    "     A2=np.dot(W2,initial_output)+B2\n",
    "     predicted_output= sigmoid(A2)\n",
    "     cache = (A1,initial_output,W1,B1, A2 ,predicted_output ,W2, B2)\n",
    "     logprobs = np.multiply(np.log(predicted_output), y) + np.multiply(np.log(1 - predicted_output), (1 - y))\n",
    "     cost = -np.sum(logprobs) / X.shape[1]\n",
    "     return cost, cache, predicted_output\n",
    "    \n",
    "\n",
    "def backwardpropogation(X,Y,cache):\n",
    "    (A1,initial_output , W1,B1, A2 , predicted_output ,W2, B2)=cache\n",
    "    dZ2=predicted_output-y\n",
    "    dW2=(np.dot(dZ2,initial_output.T)/X.shape[1])\n",
    "    dW2=  dW2 + 0.9*dW2\n",
    "    dB2=np.sum(dZ2,axis=1 ,keepdims=True)/X.shape[1]\n",
    "    \n",
    "    dA1 = np.dot(W2.T, dZ2)\n",
    "    dZ1 = np.multiply(dA1, initial_output * (1- initial_output))\n",
    "    dW1 = (np.dot(dZ1, X) / X.shape[1]) \n",
    "    dW1=  dW1 + 0.9*dW1\n",
    "    dB1 = np.sum(dZ1, axis = 1, keepdims = True) / X.shape[1]\n",
    "    \n",
    "    gradients = {\"dZ2\": dZ2, \"dW2\": dW2, \"dB2\": dB2,\n",
    "                 \"dZ1\": dZ1, \"dW1\": dW1, \"dB1\": dB1}\n",
    "    return gradients\n",
    "  \n",
    "    \n",
    "\n",
    "def updateParameters(W1,W2,B1,B2, gradients, learningRate):\n",
    "  \n",
    "    W1 =W1 - learningRate * gradients[\"dW1\"]\n",
    "    W2 =W2 - learningRate * gradients[\"dW2\"]\n",
    "    B1= B1 - learningRate * gradients[\"dB1\"]\n",
    "    B2 =B2 - learningRate * gradients[\"dB2\"]\n",
    "    return W1,W2,B1,B2\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "epoch = 86000\n",
    "learningRate = 0.15\n",
    "losses = np.zeros((epoch, 1))\n",
    "  \n",
    "for i in range(epoch):\n",
    "    losses[i, 0], cache, A2 = forwardpropogation(X, y,W1,B1,W2,B2 )\n",
    "    gradients = backwardpropogation(X, y, cache)\n",
    "    W1,W2,B1,B2 = updateParameters(W1,W2,B1,B2, gradients, learningRate)\n",
    "  \n",
    "# Evaluating the performance\n",
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"EPOCHS\")\n",
    "plt.ylabel(\"Loss value\")\n",
    "p2 = plt.show()\n",
    "\n",
    "cost, _,predicted_output = forwardpropogation(X, y, W1,B1,W2,B2)\n",
    "np.set_printoptions(suppress=True)\n",
    "print(predicted_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.array([[0,0,0,0],[0,0,0,1],[0,0,1,0],[0,0,1,1],[0,1,0,0],[0,1,0,1],[0,1,1,0],[0,1,1,1],[1,0,0,0],[1,0,0,1],[1,0,1,0],[1,0,1,1],[1,1,0,0],[1,1,0,1],[1,1,1,0],[1,1,1,1]])\n",
    "y = np.array([[0],[1],[1],[0],[1],[0],[0],[1],[1],[0],[0],[1],[0],[1],[1],[0]]).T\n",
    "\n",
    "inputneurons,hiddenlayerneurons,outputneurons=4,4,1\n",
    "W1 = np.random.uniform(-1,1, size=(hiddenlayerneurons,inputneurons))\n",
    "B1 = np.random.uniform(-1,1, size=(hiddenlayerneurons,1))\n",
    "W2 = np.random.uniform(-1,1, size=(outputneurons,hiddenlayerneurons))\n",
    "B2 = np.random.uniform(-1,1, size=(outputneurons,1)) \n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1/(1 + np.exp(-z))\n",
    "def sigmoid_derivative(z):\n",
    "    return z*(1-z)\n",
    "\n",
    "def forwardpropogation(X,y,W1,B1,W2,B2):\n",
    "     A1=np.dot(W1,X.T)+B1\n",
    "     initial_output= sigmoid(A1)\n",
    "     A2=np.dot(W2,initial_output)+B2\n",
    "     predicted_output= sigmoid(A2)\n",
    "     cache = (A1,initial_output,W1,B1, A2 ,predicted_output ,W2, B2)\n",
    "     logprobs = np.multiply(np.log(predicted_output), y) + np.multiply(np.log(1 - predicted_output), (1 - y))\n",
    "     cost = -np.sum(logprobs) / X.shape[1]\n",
    "     return cost, cache, predicted_output\n",
    "    \n",
    "\n",
    "def backwardpropogation(X,Y,cache):\n",
    "    (A1,initial_output , W1,B1, A2 , predicted_output ,W2, B2)=cache\n",
    "    dZ2=predicted_output-y\n",
    "    dW2=(np.dot(dZ2,initial_output.T)/X.shape[1])\n",
    "    dW2=  dW2 + 0.9*dW2\n",
    "    dB2=np.sum(dZ2,axis=1 ,keepdims=True)/X.shape[1]\n",
    "    \n",
    "    dA1 = np.dot(W2.T, dZ2)\n",
    "    dZ1 = np.multiply(dA1, initial_output * (1- initial_output))\n",
    "    dW1 = (np.dot(dZ1, X) / X.shape[1]) \n",
    "    dW1=  dW1 + 0.9*dW1\n",
    "    dB1 = np.sum(dZ1, axis = 1, keepdims = True) / X.shape[1]\n",
    "    \n",
    "    gradients = {\"dZ2\": dZ2, \"dW2\": dW2, \"dB2\": dB2,\n",
    "                 \"dZ1\": dZ1, \"dW1\": dW1, \"dB1\": dB1}\n",
    "    return gradients\n",
    "  \n",
    "    \n",
    "\n",
    "def updateParameters(W1,W2,B1,B2, gradients, learningRate):\n",
    "  \n",
    "    W1 =W1 - learningRate * gradients[\"dW1\"]\n",
    "    W2 =W2 - learningRate * gradients[\"dW2\"]\n",
    "    B1= B1 - learningRate * gradients[\"dB1\"]\n",
    "    B2 =B2 - learningRate * gradients[\"dB2\"]\n",
    "    return W1,W2,B1,B2\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "epoch = 70000\n",
    "learningRate = 0.20\n",
    "losses = np.zeros((epoch, 1))\n",
    "  \n",
    "for i in range(epoch):\n",
    "    losses[i, 0], cache, A2 = forwardpropogation(X, y,W1,B1,W2,B2 )\n",
    "    gradients = backwardpropogation(X, y, cache)\n",
    "    W1,W2,B1,B2 = updateParameters(W1,W2,B1,B2, gradients, learningRate)\n",
    "  \n",
    "# Evaluating the performance\n",
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"EPOCHS\")\n",
    "plt.ylabel(\"Loss value\")\n",
    "p2 = plt.show()\n",
    "\n",
    "cost, _,predicted_output = forwardpropogation(X, y, W1,B1,W2,B2)\n",
    "np.set_printoptions(suppress=True)\n",
    "print(predicted_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.array([[0,0,0,0],[0,0,0,1],[0,0,1,0],[0,0,1,1],[0,1,0,0],[0,1,0,1],[0,1,1,0],[0,1,1,1],[1,0,0,0],[1,0,0,1],[1,0,1,0],[1,0,1,1],[1,1,0,0],[1,1,0,1],[1,1,1,0],[1,1,1,1]])\n",
    "y = np.array([[0],[1],[1],[0],[1],[0],[0],[1],[1],[0],[0],[1],[0],[1],[1],[0]]).T\n",
    "\n",
    "inputneurons,hiddenlayerneurons,outputneurons=4,4,1\n",
    "W1 = np.random.uniform(-1,1, size=(hiddenlayerneurons,inputneurons))\n",
    "B1 = np.random.uniform(-1,1, size=(hiddenlayerneurons,1))\n",
    "W2 = np.random.uniform(-1,1, size=(outputneurons,hiddenlayerneurons))\n",
    "B2 = np.random.uniform(-1,1, size=(outputneurons,1)) \n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1/(1 + np.exp(-z))\n",
    "def sigmoid_derivative(z):\n",
    "    return z*(1-z)\n",
    "\n",
    "def forwardpropogation(X,y,W1,B1,W2,B2):\n",
    "     A1=np.dot(W1,X.T)+B1\n",
    "     initial_output= sigmoid(A1)\n",
    "     A2=np.dot(W2,initial_output)+B2\n",
    "     predicted_output= sigmoid(A2)\n",
    "     cache = (A1,initial_output,W1,B1, A2 ,predicted_output ,W2, B2)\n",
    "     logprobs = np.multiply(np.log(predicted_output), y) + np.multiply(np.log(1 - predicted_output), (1 - y))\n",
    "     cost = -np.sum(logprobs) / X.shape[1]\n",
    "     return cost, cache, predicted_output\n",
    "    \n",
    "\n",
    "def backwardpropogation(X,Y,cache):\n",
    "    (A1,initial_output , W1,B1, A2 , predicted_output ,W2, B2)=cache\n",
    "    dZ2=predicted_output-y\n",
    "    dW2=(np.dot(dZ2,initial_output.T)/X.shape[1])\n",
    "    dW2=  dW2 + 0.9*dW2\n",
    "    dB2=np.sum(dZ2,axis=1 ,keepdims=True)/X.shape[1]\n",
    "    \n",
    "    dA1 = np.dot(W2.T, dZ2)\n",
    "    dZ1 = np.multiply(dA1, initial_output * (1- initial_output))\n",
    "    dW1 = (np.dot(dZ1, X) / X.shape[1]) \n",
    "    dW1=  dW1 + 0.9*dW1\n",
    "    dB1 = np.sum(dZ1, axis = 1, keepdims = True) / X.shape[1]\n",
    "    \n",
    "    gradients = {\"dZ2\": dZ2, \"dW2\": dW2, \"dB2\": dB2,\n",
    "                 \"dZ1\": dZ1, \"dW1\": dW1, \"dB1\": dB1}\n",
    "    return gradients\n",
    "  \n",
    "    \n",
    "\n",
    "def updateParameters(W1,W2,B1,B2, gradients, learningRate):\n",
    "  \n",
    "    W1 =W1 - learningRate * gradients[\"dW1\"]\n",
    "    W2 =W2 - learningRate * gradients[\"dW2\"]\n",
    "    B1= B1 - learningRate * gradients[\"dB1\"]\n",
    "    B2 =B2 - learningRate * gradients[\"dB2\"]\n",
    "    return W1,W2,B1,B2\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "epoch = 60000\n",
    "learningRate = 0.25\n",
    "losses = np.zeros((epoch, 1))\n",
    "  \n",
    "for i in range(epoch):\n",
    "    losses[i, 0], cache, A2 = forwardpropogation(X, y,W1,B1,W2,B2 )\n",
    "    gradients = backwardpropogation(X, y, cache)\n",
    "    W1,W2,B1,B2 = updateParameters(W1,W2,B1,B2, gradients, learningRate)\n",
    "  \n",
    "# Evaluating the performance\n",
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"EPOCHS\")\n",
    "plt.ylabel(\"Loss value\")\n",
    "p2 = plt.show()\n",
    "\n",
    "cost, _,predicted_output = forwardpropogation(X, y, W1,B1,W2,B2)\n",
    "np.set_printoptions(suppress=True)\n",
    "print(predicted_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.array([[0,0,0,0],[0,0,0,1],[0,0,1,0],[0,0,1,1],[0,1,0,0],[0,1,0,1],[0,1,1,0],[0,1,1,1],[1,0,0,0],[1,0,0,1],[1,0,1,0],[1,0,1,1],[1,1,0,0],[1,1,0,1],[1,1,1,0],[1,1,1,1]])\n",
    "y = np.array([[0],[1],[1],[0],[1],[0],[0],[1],[1],[0],[0],[1],[0],[1],[1],[0]]).T\n",
    "\n",
    "inputneurons,hiddenlayerneurons,outputneurons=4,4,1\n",
    "W1 = np.random.uniform(-1,1, size=(hiddenlayerneurons,inputneurons))\n",
    "B1 = np.random.uniform(-1,1, size=(hiddenlayerneurons,1))\n",
    "W2 = np.random.uniform(-1,1, size=(outputneurons,hiddenlayerneurons))\n",
    "B2 = np.random.uniform(-1,1, size=(outputneurons,1)) \n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1/(1 + np.exp(-z))\n",
    "def sigmoid_derivative(z):\n",
    "    return z*(1-z)\n",
    "\n",
    "def forwardpropogation(X,y,W1,B1,W2,B2):\n",
    "     A1=np.dot(W1,X.T)+B1\n",
    "     initial_output= sigmoid(A1)\n",
    "     A2=np.dot(W2,initial_output)+B2\n",
    "     predicted_output= sigmoid(A2)\n",
    "     cache = (A1,initial_output,W1,B1, A2 ,predicted_output ,W2, B2)\n",
    "     logprobs = np.multiply(np.log(predicted_output), y) + np.multiply(np.log(1 - predicted_output), (1 - y))\n",
    "     cost = -np.sum(logprobs) / X.shape[1]\n",
    "     return cost, cache, predicted_output\n",
    "    \n",
    "\n",
    "def backwardpropogation(X,Y,cache):\n",
    "    (A1,initial_output , W1,B1, A2 , predicted_output ,W2, B2)=cache\n",
    "    dZ2=predicted_output-y\n",
    "    dW2=(np.dot(dZ2,initial_output.T)/X.shape[1])\n",
    "    dW2=  dW2 + 0.9*dW2\n",
    "    dB2=np.sum(dZ2,axis=1 ,keepdims=True)/X.shape[1]\n",
    "    \n",
    "    dA1 = np.dot(W2.T, dZ2)\n",
    "    dZ1 = np.multiply(dA1, initial_output * (1- initial_output))\n",
    "    dW1 = (np.dot(dZ1, X) / X.shape[1]) \n",
    "    dW1=  dW1 + 0.9*dW1\n",
    "    dB1 = np.sum(dZ1, axis = 1, keepdims = True) / X.shape[1]\n",
    "    \n",
    "    gradients = {\"dZ2\": dZ2, \"dW2\": dW2, \"dB2\": dB2,\n",
    "                 \"dZ1\": dZ1, \"dW1\": dW1, \"dB1\": dB1}\n",
    "    return gradients\n",
    "  \n",
    "    \n",
    "\n",
    "def updateParameters(W1,W2,B1,B2, gradients, learningRate):\n",
    "  \n",
    "    W1 =W1 - learningRate * gradients[\"dW1\"]\n",
    "    W2 =W2 - learningRate * gradients[\"dW2\"]\n",
    "    B1= B1 - learningRate * gradients[\"dB1\"]\n",
    "    B2 =B2 - learningRate * gradients[\"dB2\"]\n",
    "    return W1,W2,B1,B2\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "epoch = 68000\n",
    "learningRate = 0.30\n",
    "losses = np.zeros((epoch, 1))\n",
    "  \n",
    "for i in range(epoch):\n",
    "    losses[i, 0], cache, A2 = forwardpropogation(X, y,W1,B1,W2,B2 )\n",
    "    gradients = backwardpropogation(X, y, cache)\n",
    "    W1,W2,B1,B2 = updateParameters(W1,W2,B1,B2, gradients, learningRate)\n",
    "  \n",
    "# Evaluating the performance\n",
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"EPOCHS\")\n",
    "plt.ylabel(\"Loss value\")\n",
    "p2 = plt.show()\n",
    "\n",
    "cost, _,predicted_output = forwardpropogation(X, y, W1,B1,W2,B2)\n",
    "np.set_printoptions(suppress=True)\n",
    "print(predicted_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.array([[0,0,0,0],[0,0,0,1],[0,0,1,0],[0,0,1,1],[0,1,0,0],[0,1,0,1],[0,1,1,0],[0,1,1,1],[1,0,0,0],[1,0,0,1],[1,0,1,0],[1,0,1,1],[1,1,0,0],[1,1,0,1],[1,1,1,0],[1,1,1,1]])\n",
    "y = np.array([[0],[1],[1],[0],[1],[0],[0],[1],[1],[0],[0],[1],[0],[1],[1],[0]]).T\n",
    "\n",
    "inputneurons,hiddenlayerneurons,outputneurons=4,4,1\n",
    "W1 = np.random.uniform(-1,1, size=(hiddenlayerneurons,inputneurons))\n",
    "B1 = np.random.uniform(-1,1, size=(hiddenlayerneurons,1))\n",
    "W2 = np.random.uniform(-1,1, size=(outputneurons,hiddenlayerneurons))\n",
    "B2 = np.random.uniform(-1,1, size=(outputneurons,1)) \n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1/(1 + np.exp(-z))\n",
    "def sigmoid_derivative(z):\n",
    "    return z*(1-z)\n",
    "\n",
    "def forwardpropogation(X,y,W1,B1,W2,B2):\n",
    "     A1=np.dot(W1,X.T)+B1\n",
    "     initial_output= sigmoid(A1)\n",
    "     A2=np.dot(W2,initial_output)+B2\n",
    "     predicted_output= sigmoid(A2)\n",
    "     cache = (A1,initial_output,W1,B1, A2 ,predicted_output ,W2, B2)\n",
    "     logprobs = np.multiply(np.log(predicted_output), y) + np.multiply(np.log(1 - predicted_output), (1 - y))\n",
    "     cost = -np.sum(logprobs) / X.shape[1]\n",
    "     return cost, cache, predicted_output\n",
    "    \n",
    "\n",
    "def backwardpropogation(X,Y,cache):\n",
    "    (A1,initial_output , W1,B1, A2 , predicted_output ,W2, B2)=cache\n",
    "    dZ2=predicted_output-y\n",
    "    dW2=(np.dot(dZ2,initial_output.T)/X.shape[1])\n",
    "    dW2=  dW2 + 0.9*dW2\n",
    "    dB2=np.sum(dZ2,axis=1 ,keepdims=True)/X.shape[1]\n",
    "    \n",
    "    dA1 = np.dot(W2.T, dZ2)\n",
    "    dZ1 = np.multiply(dA1, initial_output * (1- initial_output))\n",
    "    dW1 = (np.dot(dZ1, X) / X.shape[1]) \n",
    "    dW1=  dW1 + 0.9*dW1\n",
    "    dB1 = np.sum(dZ1, axis = 1, keepdims = True) / X.shape[1]\n",
    "    \n",
    "    gradients = {\"dZ2\": dZ2, \"dW2\": dW2, \"dB2\": dB2,\n",
    "                 \"dZ1\": dZ1, \"dW1\": dW1, \"dB1\": dB1}\n",
    "    return gradients\n",
    "  \n",
    "    \n",
    "\n",
    "def updateParameters(W1,W2,B1,B2, gradients, learningRate):\n",
    "  \n",
    "    W1 =W1 - learningRate * gradients[\"dW1\"]\n",
    "    W2 =W2 - learningRate * gradients[\"dW2\"]\n",
    "    B1= B1 - learningRate * gradients[\"dB1\"]\n",
    "    B2 =B2 - learningRate * gradients[\"dB2\"]\n",
    "    return W1,W2,B1,B2\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "epoch = 66000\n",
    "learningRate = 0.35\n",
    "losses = np.zeros((epoch, 1))\n",
    "  \n",
    "for i in range(epoch):\n",
    "    losses[i, 0], cache, A2 = forwardpropogation(X, y,W1,B1,W2,B2 )\n",
    "    gradients = backwardpropogation(X, y, cache)\n",
    "    W1,W2,B1,B2 = updateParameters(W1,W2,B1,B2, gradients, learningRate)\n",
    "  \n",
    "# Evaluating the performance\n",
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"EPOCHS\")\n",
    "plt.ylabel(\"Loss value\")\n",
    "p2 = plt.show()\n",
    "\n",
    "cost, _,predicted_output = forwardpropogation(X, y, W1,B1,W2,B2)\n",
    "np.set_printoptions(suppress=True)\n",
    "print(predicted_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.array([[0,0,0,0],[0,0,0,1],[0,0,1,0],[0,0,1,1],[0,1,0,0],[0,1,0,1],[0,1,1,0],[0,1,1,1],[1,0,0,0],[1,0,0,1],[1,0,1,0],[1,0,1,1],[1,1,0,0],[1,1,0,1],[1,1,1,0],[1,1,1,1]])\n",
    "y = np.array([[0],[1],[1],[0],[1],[0],[0],[1],[1],[0],[0],[1],[0],[1],[1],[0]]).T\n",
    "\n",
    "inputneurons,hiddenlayerneurons,outputneurons=4,4,1\n",
    "W1 = np.random.uniform(-1,1, size=(hiddenlayerneurons,inputneurons))\n",
    "B1 = np.random.uniform(-1,1, size=(hiddenlayerneurons,1))\n",
    "W2 = np.random.uniform(-1,1, size=(outputneurons,hiddenlayerneurons))\n",
    "B2 = np.random.uniform(-1,1, size=(outputneurons,1)) \n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1/(1 + np.exp(-z))\n",
    "def sigmoid_derivative(z):\n",
    "    return z*(1-z)\n",
    "\n",
    "def forwardpropogation(X,y,W1,B1,W2,B2):\n",
    "     A1=np.dot(W1,X.T)+B1\n",
    "     initial_output= sigmoid(A1)\n",
    "     A2=np.dot(W2,initial_output)+B2\n",
    "     predicted_output= sigmoid(A2)\n",
    "     cache = (A1,initial_output,W1,B1, A2 ,predicted_output ,W2, B2)\n",
    "     logprobs = np.multiply(np.log(predicted_output), y) + np.multiply(np.log(1 - predicted_output), (1 - y))\n",
    "     cost = -np.sum(logprobs) / X.shape[1]\n",
    "     return cost, cache, predicted_output\n",
    "    \n",
    "\n",
    "def backwardpropogation(X,Y,cache):\n",
    "    (A1,initial_output , W1,B1, A2 , predicted_output ,W2, B2)=cache\n",
    "    dZ2=predicted_output-y\n",
    "    dW2=(np.dot(dZ2,initial_output.T)/X.shape[1])\n",
    "    dW2=  dW2 + 0.9*dW2\n",
    "    dB2=np.sum(dZ2,axis=1 ,keepdims=True)/X.shape[1]\n",
    "    \n",
    "    dA1 = np.dot(W2.T, dZ2)\n",
    "    dZ1 = np.multiply(dA1, initial_output * (1- initial_output))\n",
    "    dW1 = (np.dot(dZ1, X) / X.shape[1]) \n",
    "    dW1=  dW1 + 0.9*dW1\n",
    "    dB1 = np.sum(dZ1, axis = 1, keepdims = True) / X.shape[1]\n",
    "    \n",
    "    gradients = {\"dZ2\": dZ2, \"dW2\": dW2, \"dB2\": dB2,\n",
    "                 \"dZ1\": dZ1, \"dW1\": dW1, \"dB1\": dB1}\n",
    "    return gradients\n",
    "  \n",
    "    \n",
    "\n",
    "def updateParameters(W1,W2,B1,B2, gradients, learningRate):\n",
    "  \n",
    "    W1 =W1 - learningRate * gradients[\"dW1\"]\n",
    "    W2 =W2 - learningRate * gradients[\"dW2\"]\n",
    "    B1= B1 - learningRate * gradients[\"dB1\"]\n",
    "    B2 =B2 - learningRate * gradients[\"dB2\"]\n",
    "    return W1,W2,B1,B2\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "epoch = 40000\n",
    "learningRate = 0.40\n",
    "losses = np.zeros((epoch, 1))\n",
    "  \n",
    "for i in range(epoch):\n",
    "    losses[i, 0], cache, A2 = forwardpropogation(X, y,W1,B1,W2,B2 )\n",
    "    gradients = backwardpropogation(X, y, cache)\n",
    "    W1,W2,B1,B2 = updateParameters(W1,W2,B1,B2, gradients, learningRate)\n",
    "  \n",
    "# Evaluating the performance\n",
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"EPOCHS\")\n",
    "plt.ylabel(\"Loss value\")\n",
    "p2 = plt.show()\n",
    "\n",
    "cost, _,predicted_output = forwardpropogation(X, y, W1,B1,W2,B2)\n",
    "np.set_printoptions(suppress=True)\n",
    "print(predicted_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.array([[0,0,0,0],[0,0,0,1],[0,0,1,0],[0,0,1,1],[0,1,0,0],[0,1,0,1],[0,1,1,0],[0,1,1,1],[1,0,0,0],[1,0,0,1],[1,0,1,0],[1,0,1,1],[1,1,0,0],[1,1,0,1],[1,1,1,0],[1,1,1,1]])\n",
    "y = np.array([[0],[1],[1],[0],[1],[0],[0],[1],[1],[0],[0],[1],[0],[1],[1],[0]]).T\n",
    "\n",
    "inputneurons,hiddenlayerneurons,outputneurons=4,4,1\n",
    "W1 = np.random.uniform(-1,1, size=(hiddenlayerneurons,inputneurons))\n",
    "B1 = np.random.uniform(-1,1, size=(hiddenlayerneurons,1))\n",
    "W2 = np.random.uniform(-1,1, size=(outputneurons,hiddenlayerneurons))\n",
    "B2 = np.random.uniform(-1,1, size=(outputneurons,1)) \n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1/(1 + np.exp(-z))\n",
    "def sigmoid_derivative(z):\n",
    "    return z*(1-z)\n",
    "\n",
    "def forwardpropogation(X,y,W1,B1,W2,B2):\n",
    "     A1=np.dot(W1,X.T)+B1\n",
    "     initial_output= sigmoid(A1)\n",
    "     A2=np.dot(W2,initial_output)+B2\n",
    "     predicted_output= sigmoid(A2)\n",
    "     cache = (A1,initial_output,W1,B1, A2 ,predicted_output ,W2, B2)\n",
    "     logprobs = np.multiply(np.log(predicted_output), y) + np.multiply(np.log(1 - predicted_output), (1 - y))\n",
    "     cost = -np.sum(logprobs) / X.shape[1]\n",
    "     return cost, cache, predicted_output\n",
    "    \n",
    "\n",
    "def backwardpropogation(X,Y,cache):\n",
    "    (A1,initial_output , W1,B1, A2 , predicted_output ,W2, B2)=cache\n",
    "    dZ2=predicted_output-y\n",
    "    dW2=(np.dot(dZ2,initial_output.T)/X.shape[1])\n",
    "    dW2=  dW2 + 0.9*dW2\n",
    "    dB2=np.sum(dZ2,axis=1 ,keepdims=True)/X.shape[1]\n",
    "    \n",
    "    dA1 = np.dot(W2.T, dZ2)\n",
    "    dZ1 = np.multiply(dA1, initial_output * (1- initial_output))\n",
    "    dW1 = (np.dot(dZ1, X) / X.shape[1]) \n",
    "    dW1=  dW1 + 0.9*dW1\n",
    "    dB1 = np.sum(dZ1, axis = 1, keepdims = True) / X.shape[1]\n",
    "    \n",
    "    gradients = {\"dZ2\": dZ2, \"dW2\": dW2, \"dB2\": dB2,\n",
    "                 \"dZ1\": dZ1, \"dW1\": dW1, \"dB1\": dB1}\n",
    "    return gradients\n",
    "  \n",
    "    \n",
    "\n",
    "def updateParameters(W1,W2,B1,B2, gradients, learningRate):\n",
    "  \n",
    "    W1 =W1 - learningRate * gradients[\"dW1\"]\n",
    "    W2 =W2 - learningRate * gradients[\"dW2\"]\n",
    "    B1= B1 - learningRate * gradients[\"dB1\"]\n",
    "    B2 =B2 - learningRate * gradients[\"dB2\"]\n",
    "    return W1,W2,B1,B2\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "epoch = 30000\n",
    "learningRate = 0.45\n",
    "losses = np.zeros((epoch, 1))\n",
    "  \n",
    "for i in range(epoch):\n",
    "    losses[i, 0], cache, A2 = forwardpropogation(X, y,W1,B1,W2,B2 )\n",
    "    gradients = backwardpropogation(X, y, cache)\n",
    "    W1,W2,B1,B2 = updateParameters(W1,W2,B1,B2, gradients, learningRate)\n",
    "  \n",
    "# Evaluating the performance\n",
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"EPOCHS\")\n",
    "plt.ylabel(\"Loss value\")\n",
    "p2 = plt.show()\n",
    "\n",
    "cost, _,predicted_output = forwardpropogation(X, y, W1,B1,W2,B2)\n",
    "np.set_printoptions(suppress=True)\n",
    "print(predicted_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.array([[0,0,0,0],[0,0,0,1],[0,0,1,0],[0,0,1,1],[0,1,0,0],[0,1,0,1],[0,1,1,0],[0,1,1,1],[1,0,0,0],[1,0,0,1],[1,0,1,0],[1,0,1,1],[1,1,0,0],[1,1,0,1],[1,1,1,0],[1,1,1,1]])\n",
    "y = np.array([[0],[1],[1],[0],[1],[0],[0],[1],[1],[0],[0],[1],[0],[1],[1],[0]]).T\n",
    "\n",
    "inputneurons,hiddenlayerneurons,outputneurons=4,4,1\n",
    "W1 = np.random.uniform(-1,1, size=(hiddenlayerneurons,inputneurons))\n",
    "B1 = np.random.uniform(-1,1, size=(hiddenlayerneurons,1))\n",
    "W2 = np.random.uniform(-1,1, size=(outputneurons,hiddenlayerneurons))\n",
    "B2 = np.random.uniform(-1,1, size=(outputneurons,1)) \n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1/(1 + np.exp(-z))\n",
    "def sigmoid_derivative(z):\n",
    "    return z*(1-z)\n",
    "\n",
    "def forwardpropogation(X,y,W1,B1,W2,B2):\n",
    "     A1=np.dot(W1,X.T)+B1\n",
    "     initial_output= sigmoid(A1)\n",
    "     A2=np.dot(W2,initial_output)+B2\n",
    "     predicted_output= sigmoid(A2)\n",
    "     cache = (A1,initial_output,W1,B1, A2 ,predicted_output ,W2, B2)\n",
    "     logprobs = np.multiply(np.log(predicted_output), y) + np.multiply(np.log(1 - predicted_output), (1 - y))\n",
    "     cost = -np.sum(logprobs) / X.shape[1]\n",
    "     return cost, cache, predicted_output\n",
    "    \n",
    "\n",
    "def backwardpropogation(X,Y,cache):\n",
    "    (A1,initial_output , W1,B1, A2 , predicted_output ,W2, B2)=cache\n",
    "    dZ2=predicted_output-y\n",
    "    dW2=(np.dot(dZ2,initial_output.T)/X.shape[1])\n",
    "    dW2=  dW2 + 0.9*dW2\n",
    "    dB2=np.sum(dZ2,axis=1 ,keepdims=True)/X.shape[1]\n",
    "    \n",
    "    dA1 = np.dot(W2.T, dZ2)\n",
    "    dZ1 = np.multiply(dA1, initial_output * (1- initial_output))\n",
    "    dW1 = (np.dot(dZ1, X) / X.shape[1]) \n",
    "    dW1=  dW1 + 0.9*dW1\n",
    "    dB1 = np.sum(dZ1, axis = 1, keepdims = True) / X.shape[1]\n",
    "    \n",
    "    gradients = {\"dZ2\": dZ2, \"dW2\": dW2, \"dB2\": dB2,\n",
    "                 \"dZ1\": dZ1, \"dW1\": dW1, \"dB1\": dB1}\n",
    "    return gradients\n",
    "  \n",
    "    \n",
    "\n",
    "def updateParameters(W1,W2,B1,B2, gradients, learningRate):\n",
    "  \n",
    "    W1 =W1 - learningRate * gradients[\"dW1\"]\n",
    "    W2 =W2 - learningRate * gradients[\"dW2\"]\n",
    "    B1= B1 - learningRate * gradients[\"dB1\"]\n",
    "    B2 =B2 - learningRate * gradients[\"dB2\"]\n",
    "    return W1,W2,B1,B2\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "epoch = 25000\n",
    "learningRate = 0.50\n",
    "losses = np.zeros((epoch, 1))\n",
    "  \n",
    "for i in range(epoch):\n",
    "    losses[i, 0], cache, A2 = forwardpropogation(X, y,W1,B1,W2,B2 )\n",
    "    gradients = backwardpropogation(X, y, cache)\n",
    "    W1,W2,B1,B2 = updateParameters(W1,W2,B1,B2, gradients, learningRate)\n",
    "  \n",
    "# Evaluating the performance\n",
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"EPOCHS\")\n",
    "plt.ylabel(\"Loss value\")\n",
    "p2 = plt.show()\n",
    "\n",
    "cost, _,predicted_output = forwardpropogation(X, y, W1,B1,W2,B2)\n",
    "np.set_printoptions(suppress=True)\n",
    "print(predicted_output)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Convergence for lr= 0.05, occurs with epoch value around 800000 whereas with lr= 0.1, convergence occurs the value of epochs being 500000.Lesser number of epoches are required to achieve convergence with higher learning rate. It can be observed that as the learninng rate increases convergence can be obtained with less number of epochs.  It can also be observed that loss value can be reduced with increase in the number of epochs.\n",
    "\n",
    "## For η = 0.05 ,convergence is at 800000 epochs and after adding the momentum with α = 0.9 , convergence is at 100000 epochs. The performance of training has improved about 8 times.\n",
    "## For η = 0.1 ,convergence is at 500000 epochs and after adding the momentum with α = 0.9 , convergence is at 90000 epochs. The performance of training has improved about 5.5 times.\n",
    "## For η = 0.15 ,convergence is at 400000 epochs and after adding the momentum with α = 0.9 , convergence is at 86000 epochs. The performance of training has improved about 4.65 times.\n",
    "## For η = 0.20 ,convergence is at 300000 epochs and after adding the momentum with α = 0.9 , convergence is at 70000 epochs. The performance of training has improved about 3.48 times.\n",
    "## For η = 0.25 ,convergence is at 200000 epochs and after adding the momentum with α = 0.9 , convergence is at 60000 epochs. The performance of training has improved about 3.3 times.\n",
    "## For η = 0.30 ,convergence is at 100000 epochs and after adding the momentum with α = 0.9 , convergence is at 68000 epochs. The performance of training has improved about 1.47 times.\n",
    "## For η = 0.35 ,convergence is at 90000 epochs and after adding the momentum with α = 0.9 , convergence is at 66000 epochs. The performance of training has improved about 1.36 times.\n",
    "## For η = 0.40 ,convergence is at 80000 epochs and after adding the momentum with α = 0.9 , convergence is at 40000 epochs. The performance of training has improved about 2 times.\n",
    "## For η = 0.45 ,convergence is at 70000 epochs and after adding the momentum with α = 0.9 , convergence is at 30000 epochs. The performance of training has improved about 2.3 times.\n",
    "## For η = 0.50 ,convergence is at 60000 epochs and after adding the momentum with α = 0.9 , convergence is at 25000 epochs. The performance of training has improved about 2.4 times.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
